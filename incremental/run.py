# -*- coding: utf-8 -*-
import os
import math
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.metrics import classification_report, confusion_matrix
from torch.utils.data import DataLoader, Dataset
from torch.optim import SGD, AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from contextlib import contextmanager
import random
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime

# === å¯¼å…¥æ‚¨çš„æ¨¡å‹ ===
from models_adapted import OpenEmbed


# ---------------------- å…¨å±€ç¨³å®šæ€§ï¼ˆå¯å¤ç°å®éªŒï¼‰ ----------------------
def set_global_seed(seed: int = 42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = True


set_global_seed(42)

# å¯é€‰å¢å¼ºï¼šè‹¥ data_utils ç¼ºå¤±åˆ™å…œåº•ä¸ºæ’ç­‰
try:
    from utils import signal_awgn, freq_perturb, time_scaling
except Exception:
    def signal_awgn(x):
        return x


    def freq_perturb(x):
        return x


    def time_scaling(x):
        return x


# ======================= é…ç½® =======================
class Cfg:
    # è·¯å¾„
    data_root = "../data/ustc_all"
    teacher_model_path = "ustc_model/15_ustcALL_0.pt"  # é¢„è®­ç»ƒçš„15ç±»æ¨¡å‹
    new_class_data_paths = (
        "../data/ustc_all/15.csv",
        "../data/ustc_all/16.csv",
        "../data/ustc_all/17.csv",
        "../data/ustc_all/18.csv",
        "../data/ustc_all/19.csv",
    )
    out_dir = "./checkpoints_ustc"
    final_model = os.path.join(out_dir, "gem_final.pth")

    # å¯è§†åŒ–ä¿å­˜è·¯å¾„
    vis_dir = "./visualizations"
    loss_plot_path = os.path.join(vis_dir, "training_loss.png")
    accuracy_plot_path = os.path.join(vis_dir, "accuracy_evolution.png")
    confusion_matrix_path = os.path.join(vis_dir, "confusion_matrix.png")

    # ç±»åˆ«
    known_classes = tuple(range(15))  # 0-14ä¸ºå·²çŸ¥ç±»
    new_class_labels = (15, 16, 17, 18, 19)  # æ–°å¢5ä¸ªç±»åˆ«
    _unknown_for_loader = (15, 19)  # ç”¨äºæ•°æ®åŠ è½½å™¨æ¥å£å…¼å®¹

    # è®­ç»ƒ
    epochs = 10
    batch_size = 1024
    device = "cuda" if torch.cuda.is_available() else "cpu"
    num_workers = 0
    pin_memory = True
    print_every = 20
    do_eval_during_train = True
    eval_every = 1

    # ä¼˜åŒ–å™¨
    optimizer_type = "adamw"  # 'sgd' or 'adamw'
    lr = 1e-3
    weight_decay = 0.0  # å»ºè®® 0ï¼šé¿å…ç ´å GEM å‡ ä½•çº¦æŸ
    momentum = 0.9
    nesterov = True
    label_smoothing = 0.1

    # GEM å‚æ•°
    gem_margin = 0.5  # å¯è¯• 5e-4 ~ 1e-3 å¢å¼ºä¸å¯é—å¿˜ç¡¬åº¦
    gem_num_ref_batches = 5  # K=1 èµ°é—­å¼è§£ï¼›>1 ä¼šèµ° QP/PGD

    # è®°å¿†æ± ï¼ˆä»æ—§ç±»å…¨é‡è®­ç»ƒé›†å‡åŒ€é‡‡æ ·ï¼‰
    memory_per_class = 100

    # è¡Œä¸ºå¼€å…³
    freeze_old_rows_with_hook = False  # True: åªè®­æ–°å¢ç±»åˆ«è¡Œ
    show_teacher_upper_bound = True  # æ‰“å°æ•™å¸ˆæ¨¡å‹æ—§ç±»ä¸Šé™


# ======================= å¯è§†åŒ–å·¥å…· =======================
class Visualization:
    def __init__(self, cfg: Cfg):
        self.cfg = cfg
        self.training_losses = []
        self.eval_metrics = []  # ä¿å­˜æ¯ä¸ªepochçš„è¯„ä¼°ç»“æœ
        self.epochs_list = []

        # åˆ›å»ºå¯è§†åŒ–ç›®å½•
        os.makedirs(cfg.vis_dir, exist_ok=True)

    def add_training_loss(self, epoch, loss):
        """è®°å½•è®­ç»ƒæŸå¤±"""
        self.training_losses.append((epoch, loss))

    def add_eval_metrics(self, epoch, metrics):
        """è®°å½•è¯„ä¼°æŒ‡æ ‡"""
        self.eval_metrics.append((epoch, metrics))
        self.epochs_list.append(epoch)

    def plot_training_loss(self):
        """ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿"""
        if not self.training_losses:
            print("æ²¡æœ‰è®­ç»ƒæŸå¤±æ•°æ®å¯ç»˜åˆ¶")
            return

        epochs = [e for e, _ in self.training_losses]
        losses = [l for _, l in self.training_losses]

        plt.figure(figsize=(10, 6))
        plt.plot(epochs, losses, 'b-', linewidth=2, label='Training Loss')
        plt.xlabel('Epoch', fontsize=12)
        plt.ylabel('Loss', fontsize=12)
        plt.title('Training Loss Curve', fontsize=14, fontweight='bold')
        plt.grid(True, alpha=0.3)
        plt.legend(fontsize=12)
        plt.tight_layout()
        plt.savefig(self.cfg.loss_plot_path, dpi=150, bbox_inches='tight')
        plt.show()
        print(f"è®­ç»ƒæŸå¤±æ›²çº¿å·²ä¿å­˜åˆ°: {self.cfg.loss_plot_path}")

    def plot_accuracy_evolution(self):
        """ç»˜åˆ¶å‡†ç¡®ç‡æ¼”åŒ–æ›²çº¿"""
        if not self.eval_metrics:
            print("æ²¡æœ‰è¯„ä¼°æŒ‡æ ‡æ•°æ®å¯ç»˜åˆ¶")
            return

        epochs = self.epochs_list
        old_acc = [metrics['old'] for _, metrics in self.eval_metrics]
        new_acc = [metrics['new'] for _, metrics in self.eval_metrics]
        overall_acc = [metrics['overall'] for _, metrics in self.eval_metrics]

        plt.figure(figsize=(12, 7))
        plt.plot(epochs, old_acc, 'r-o', linewidth=2, markersize=8, label='Old Classes Accuracy')
        plt.plot(epochs, new_acc, 'g-s', linewidth=2, markersize=8, label='New Classes Accuracy')
        plt.plot(epochs, overall_acc, 'b-^', linewidth=2, markersize=8, label='Overall Accuracy')

        plt.xlabel('Epoch', fontsize=12)
        plt.ylabel('Accuracy', fontsize=12)
        plt.title('Accuracy Evolution During Training', fontsize=14, fontweight='bold')
        plt.grid(True, alpha=0.3)
        plt.legend(fontsize=12, loc='best')
        plt.ylim([0, 1.05])

        # æ·»åŠ å‡†ç¡®ç‡æ•°å€¼æ ‡æ³¨
        for i, (old, new, overall) in enumerate(zip(old_acc, new_acc, overall_acc)):
            if i % 2 == 0:  # æ¯éš”ä¸€ä¸ªepochæ ‡æ³¨ä¸€æ¬¡
                plt.annotate(f'{old:.3f}', (epochs[i], old_acc[i]), textcoords="offset points",
                             xytext=(0, 10), ha='center', fontsize=9, color='red')
                plt.annotate(f'{new:.3f}', (epochs[i], new_acc[i]), textcoords="offset points",
                             xytext=(0, -15), ha='center', fontsize=9, color='green')

        plt.tight_layout()
        plt.savefig(self.cfg.accuracy_plot_path, dpi=150, bbox_inches='tight')
        plt.show()
        print(f"å‡†ç¡®ç‡æ¼”åŒ–æ›²çº¿å·²ä¿å­˜åˆ°: {self.cfg.accuracy_plot_path}")

    def plot_confusion_matrix(self, model, test_old_loader, new_test_loader, device):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        model.eval()
        all_preds = []
        all_labels = []

        # æ”¶é›†æ—§ç±»é¢„æµ‹
        with torch.no_grad():
            for x, y in test_old_loader:
                x, y = x.to(device), y.to(device)
                _, logits = model(x)
                preds = logits.argmax(1)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(y.cpu().numpy())

        # æ”¶é›†æ–°ç±»é¢„æµ‹
        with torch.no_grad():
            for x, y in new_test_loader:
                x, y = x.to(device), y.to(device)
                _, logits = model(x)
                preds = logits.argmax(1)
                all_preds.extend(preds.cpu().numpy())
                all_labels.extend(y.cpu().numpy())

        # åˆ›å»ºæ··æ·†çŸ©é˜µ
        all_preds = np.array(all_preds)
        all_labels = np.array(all_labels)

        # è®¡ç®—æ··æ·†çŸ©é˜µ
        cm = confusion_matrix(all_labels, all_preds)

        # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
        plt.figure(figsize=(14, 12))

        # åˆ›å»ºç±»åˆ«æ ‡ç­¾
        num_old = len(self.cfg.known_classes)
        num_new = len(self.cfg.new_class_labels)
        total_classes = num_old + num_new
        class_names = [f'Old-{i}' for i in range(num_old)] + [f'New-{i}' for i in self.cfg.new_class_labels]

        # ä½¿ç”¨seabornç»˜åˆ¶çƒ­åŠ›å›¾
        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                    xticklabels=class_names, yticklabels=class_names,
                    cbar_kws={'label': 'Count'})

        plt.xlabel('Predicted Label', fontsize=12)
        plt.ylabel('True Label', fontsize=12)
        plt.title('Confusion Matrix', fontsize=14, fontweight='bold')

        # æ·»åŠ åˆ†éš”çº¿åŒºåˆ†æ–°æ—§ç±»åˆ«
        plt.axhline(y=num_old, color='red', linewidth=2, linestyle='--', alpha=0.7)
        plt.axvline(x=num_old, color='red', linewidth=2, linestyle='--', alpha=0.7)

        plt.tight_layout()
        plt.savefig(self.cfg.confusion_matrix_path, dpi=150, bbox_inches='tight')
        plt.show()
        print(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: {self.cfg.confusion_matrix_path}")

        return cm

    def print_final_report(self, final_metrics, confusion_mat):
        """æ‰“å°æœ€ç»ˆè®­ç»ƒæŠ¥å‘Š"""
        print("\n" + "=" * 80)
        print("è®­ç»ƒå®Œæˆ - æœ€ç»ˆæŠ¥å‘Š")
        print("=" * 80)

        # åŸºæœ¬ä¿¡æ¯
        print(f"\nğŸ“Š æ¨¡å‹æ€§èƒ½æŒ‡æ ‡:")
        print(f"   æ—§ç±»åˆ«å‡†ç¡®ç‡: {final_metrics['old']:.4f} ({final_metrics['old'] * 100:.2f}%)")
        print(f"   æ–°ç±»åˆ«å‡†ç¡®ç‡: {final_metrics['new']:.4f} ({final_metrics['new'] * 100:.2f}%)")
        print(f"   æ€»ä½“å‡†ç¡®ç‡:   {final_metrics['overall']:.4f} ({final_metrics['overall'] * 100:.2f}%)")

        # æ··æ·†çŸ©é˜µç»Ÿè®¡
        print(f"\nğŸ“ˆ æ··æ·†çŸ©é˜µç»Ÿè®¡:")
        print(f"   æ€»æ ·æœ¬æ•°: {confusion_mat.sum()}")
        print(f"   æ­£ç¡®åˆ†ç±»æ•°: {np.trace(confusion_mat)}")
        print(f"   æ•´ä½“å‡†ç¡®ç‡: {np.trace(confusion_mat) / confusion_mat.sum():.4f}")

        # æ–°æ—§ç±»åˆ«åˆ†ç¦»ç»Ÿè®¡
        num_old = len(self.cfg.known_classes)
        old_old = confusion_mat[:num_old, :num_old].sum()
        old_new = confusion_mat[:num_old, num_old:].sum()
        new_old = confusion_mat[num_old:, :num_old].sum()
        new_new = confusion_mat[num_old:, num_old:].sum()

        print(f"\nğŸ” æ–°æ—§ç±»åˆ«äº¤å‰åˆ†æ:")
        print(f"   æ—§ç±»è¢«æ­£ç¡®è¯†åˆ«ä¸ºæ—§ç±»: {old_old}")
        print(f"   æ—§ç±»è¢«è¯¯è¯†åˆ«ä¸ºæ–°ç±»:   {old_new}")
        print(f"   æ–°ç±»è¢«è¯¯è¯†åˆ«ä¸ºæ—§ç±»:   {new_old}")
        print(f"   æ–°ç±»è¢«æ­£ç¡®è¯†åˆ«ä¸ºæ–°ç±»: {new_new}")

        # ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_path = os.path.join(self.cfg.vis_dir, "training_report.txt")
        with open(report_path, 'w') as f:
            f.write("è®­ç»ƒå®ŒæˆæŠ¥å‘Š\n")
            f.write("=" * 50 + "\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"è®­ç»ƒè½®æ¬¡: {self.cfg.epochs}\n")
            f.write(f"å·²çŸ¥ç±»åˆ«æ•°: {len(self.cfg.known_classes)}\n")
            f.write(f"æ–°å¢ç±»åˆ«æ•°: {len(self.cfg.new_class_labels)}\n")
            f.write(f"è®°å¿†æ ·æœ¬/ç±»: {self.cfg.memory_per_class}\n\n")

            f.write("æ€§èƒ½æŒ‡æ ‡:\n")
            f.write(f"  æ—§ç±»åˆ«å‡†ç¡®ç‡: {final_metrics['old']:.4f}\n")
            f.write(f"  æ–°ç±»åˆ«å‡†ç¡®ç‡: {final_metrics['new']:.4f}\n")
            f.write(f"  æ€»ä½“å‡†ç¡®ç‡:   {final_metrics['overall']:.4f}\n\n")

            f.write("å¯è§†åŒ–æ–‡ä»¶:\n")
            f.write(f"  æŸå¤±æ›²çº¿: {self.cfg.loss_plot_path}\n")
            f.write(f"  å‡†ç¡®ç‡æ›²çº¿: {self.cfg.accuracy_plot_path}\n")
            f.write(f"  æ··æ·†çŸ©é˜µ: {self.cfg.confusion_matrix_path}\n")

        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")


# ======================= æ•°æ®åŠ è½½å’Œå¤„ç† =======================
def load_csv_data(file_path):
    """åŠ è½½CSVæ–‡ä»¶æ•°æ®ï¼Œå¤„ç†æ··åˆç±»å‹"""
    try:
        # æ–¹æ³•1: å°è¯•è‡ªåŠ¨è½¬æ¢ç±»å‹
        df = pd.read_csv(file_path, header=None, low_memory=False)

        # æ¸…ç†æ•°æ®ï¼šå°†éæ•°å€¼æ•°æ®è½¬æ¢ä¸ºNaNï¼Œç„¶åå¡«å……ä¸º0
        for col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

        # å¡«å……NaNå€¼ä¸º0
        df = df.fillna(0)

        data = df.values.astype(np.float32)

        print(f"Loaded {file_path}: shape {data.shape}")
        return data

    except Exception as e:
        print(f"Error loading {file_path}: {e}")
        # æ–¹æ³•2: å¦‚æœè‡ªåŠ¨è½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨æ›´ç¨³å¥çš„æ–¹æ³•
        try:
            data_list = []
            with open(file_path, 'r') as f:
                for line in f:
                    # åˆ†å‰²æ¯è¡Œçš„æ•°æ®
                    parts = line.strip().split(',')
                    row_data = []
                    for part in parts:
                        try:
                            # å°è¯•è½¬æ¢ä¸ºfloat
                            val = float(part)
                            row_data.append(val)
                        except ValueError:
                            # å¦‚æœè½¬æ¢å¤±è´¥ï¼Œä½¿ç”¨0
                            row_data.append(0.0)
                    data_list.append(row_data)

            # ç¡®ä¿æ‰€æœ‰è¡Œé•¿åº¦ä¸€è‡´
            max_len = max(len(row) for row in data_list)
            for i in range(len(data_list)):
                while len(data_list[i]) < max_len:
                    data_list[i].append(0.0)
                data_list[i] = data_list[i][:max_len]  # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦

            data = np.array(data_list, dtype=np.float32)
            print(f"Loaded {file_path} with fallback method: shape {data.shape}")
            return data

        except Exception as e2:
            print(f"Fallback method also failed for {file_path}: {e2}")
            return np.empty((0, 784), dtype=np.float32)  # è¿”å›ç©ºæ•°ç»„


def load_all_data(data_root, classes):
    """åŠ è½½æŒ‡å®šç±»åˆ«çš„æ‰€æœ‰æ•°æ®"""
    X_list, y_list = [], []
    for cls in classes:
        file_path = os.path.join(data_root, f"{cls}.csv")
        if os.path.exists(file_path):
            data = load_csv_data(file_path)
            if len(data) > 0:
                X_list.append(data)
                y_list.append(np.full((data.shape[0],), cls, dtype=np.int64))
                print(f"Class {cls}: loaded {data.shape[0]} samples")
            else:
                print(f"Warning: No data loaded from {file_path}")
        else:
            print(f"Warning: {file_path} not found")

    if len(X_list) == 0:
        print("Error: No data loaded for any class!")
        return np.empty((0, 784)), np.empty((0,), dtype=np.int64)

    X = np.vstack(X_list)
    y = np.hstack(y_list)

    print(f"Total loaded: {X.shape[0]} samples, {y.shape[0]} labels")
    return X, y


# ======================= å®ç”¨ï¼šå®‰å…¨å¢å¼º + Dataset =======================
def _to_tensor(x):
    return x if isinstance(x, torch.Tensor) else torch.tensor(x, dtype=torch.float32)


def safe_augment(x: torch.Tensor) -> torch.Tensor:
    x = _to_tensor(x).to(torch.float32)
    x = _to_tensor(signal_awgn(x)).to(torch.float32)
    x = _to_tensor(freq_perturb(x)).to(torch.float32)
    x = _to_tensor(time_scaling(x)).to(torch.float32)
    return x


class SignalDataset(Dataset):
    def __init__(self, X, y, transform=None):
        if isinstance(X, np.ndarray):
            X = torch.from_numpy(X).float()
        if isinstance(y, np.ndarray):
            y = torch.from_numpy(y).long()

        self.X = X.detach().cpu().float()
        self.y = y.detach().cpu().long()
        self.tf = transform

        # ç¡®ä¿æ•°æ®å½¢çŠ¶æ­£ç¡® (batch_size, 784)
        if self.X.dim() == 1:
            self.X = self.X.unsqueeze(0)

        # å¦‚æœæ•°æ®é•¿åº¦ä¸æ˜¯784ï¼Œè¿›è¡Œè°ƒæ•´
        if self.X.shape[1] > 784:
            self.X = self.X[:, :784]  # æˆªæ–­
        elif self.X.shape[1] < 784:
            # å¡«å……åˆ°784
            padding = torch.zeros(self.X.shape[0], 784 - self.X.shape[1])
            self.X = torch.cat([self.X, padding], dim=1)

        print(f"Dataset: X shape {self.X.shape}, y shape {self.y.shape}")

    def __len__(self):
        return self.X.shape[0]

    def __getitem__(self, i):
        x, y = self.X[i], self.y[i]
        if self.tf is not None:
            try:
                x = self.tf(x)
            except Exception:
                pass
        return x, y


# ======================= Data æ„å»ºï¼ˆæ–°æ•°æ® + æ—§ç±»è®°å¿† + æµ‹è¯•ï¼‰ =======================
def build_loaders_and_memory(cfg: Cfg):
    # === æ–°ç±»æ•´æ± ï¼šæŠŠæ–°ç°‡æ ‡ä¸º [num_old, num_old+1, ...] ===
    X_new, y_new = [], []
    num_old = len(cfg.known_classes)

    print("Loading new class data...")
    for i, p in enumerate(cfg.new_class_data_paths):
        if os.path.exists(p):
            arr = load_csv_data(p)
            if len(arr) > 0:
                X_new.append(torch.from_numpy(arr).float())
                y_new.append(torch.full((arr.shape[0],), num_old + i, dtype=torch.long))
                print(f"New class {num_old + i}: {arr.shape[0]} samples")
        else:
            print(f"Warning: {p} not found")

    if len(X_new) == 0:
        raise ValueError("No new class data found!")

    X_new = torch.cat(X_new, 0)
    y_new = torch.cat(y_new, 0)

    new_train_loader = DataLoader(
        SignalDataset(X_new, y_new, transform=safe_augment),
        batch_size=cfg.batch_size, shuffle=True,
        num_workers=cfg.num_workers, pin_memory=cfg.pin_memory, drop_last=False
    )

    # === æ—§ç±»è®­ç»ƒå…¨é›† & æµ‹è¯•é›† ===
    print("Loading old class data...")
    X_old_train, y_old_train = load_all_data(cfg.data_root, cfg.known_classes)

    if len(X_old_train) == 0:
        raise ValueError("No old class data found!")

    # åˆ†å‰²è®­ç»ƒæµ‹è¯•é›† (8:2)
    from sklearn.model_selection import train_test_split
    X_old_train, X_old_test, y_old_train, y_old_test = train_test_split(
        X_old_train, y_old_train, test_size=0.2, random_state=42, stratify=y_old_train
    )

    old_train_loader = DataLoader(
        SignalDataset(X_old_train, y_old_train),
        batch_size=cfg.batch_size, shuffle=True,
        num_workers=cfg.num_workers, pin_memory=cfg.pin_memory
    )

    old_test_loader = DataLoader(
        SignalDataset(X_old_test, y_old_test),
        batch_size=cfg.batch_size, shuffle=False,
        num_workers=cfg.num_workers, pin_memory=cfg.pin_memory
    )

    # === æ–°ç±»æµ‹è¯•é›† ===
    print("Creating new class test set...")
    X_new_test, y_new_test = [], []
    for i, p in enumerate(cfg.new_class_data_paths):
        if os.path.exists(p):
            arr = load_csv_data(p)
            if len(arr) > 0:
                # å–20%ä½œä¸ºæµ‹è¯•
                if len(arr) > 10:  # ç¡®ä¿æœ‰è¶³å¤Ÿæ ·æœ¬
                    arr_train, arr_test = train_test_split(arr, test_size=0.2, random_state=42)
                    X_new_test.append(torch.from_numpy(arr_test).float())
                    y_new_test.append(torch.full((arr_test.shape[0],), num_old + i, dtype=torch.long))

    if len(X_new_test) > 0:
        X_new_test = torch.cat(X_new_test, 0)
        y_new_test = torch.cat(y_new_test, 0)
        new_test_loader = DataLoader(
            SignalDataset(X_new_test, y_new_test),
            batch_size=cfg.batch_size, shuffle=False,
            num_workers=cfg.num_workers, pin_memory=cfg.pin_memory
        )
        print(f"New test set: {X_new_test.shape[0]} samples")
    else:
        print("Warning: Using old test loader as fallback for new test set")
        new_test_loader = old_test_loader

    # === è®°å¿†æ± ï¼ˆæ¯æ—§ç±»éšæœºé‡‡æ · memory_per_classï¼›å¸¸é©» CPUï¼‰ ===
    print("Building memory pool...")
    by_cls = {}
    for xb, yb in old_train_loader:
        for i in range(len(yb)):
            c = int(yb[i].item())
            by_cls.setdefault(c, []).append(xb[i].detach().cpu())

    mem_X_list, mem_y_list = [], []
    for c, xs in by_cls.items():
        n = len(xs)
        if n == 0:
            continue
        k = min(cfg.memory_per_class, n)
        idx = np.random.choice(n, k, replace=False)
        mem_X_list += [xs[j] for j in idx]
        mem_y_list += [c] * k

    if len(mem_X_list) > 0:
        mem_X = torch.stack(mem_X_list).contiguous()  # CPU å¸¸é©»
        mem_y = torch.tensor(mem_y_list, dtype=torch.long)  # CPU
        # å®‰å…¨é‡æ˜ å°„ï¼ˆè‹¥æ ‡ç­¾ä¸æ˜¯ 0..num_old-1ï¼‰
        if mem_y.numel() > 0 and (int(mem_y.max()) >= num_old or int(mem_y.min()) < 0):
            cls2idx = {c: i for i, c in enumerate(cfg.known_classes)}
            mem_y = torch.tensor([cls2idx[int(c)] for c in mem_y.tolist()], dtype=torch.long)
    else:
        mem_X = torch.empty(0)
        mem_y = torch.empty(0, dtype=torch.long)

    print(f"[Data] æ–°ç±»è®­ç»ƒ: {X_new.shape[0]} | æ—§ç±»è®°å¿†: {mem_X.shape[0]}")
    return new_train_loader, new_test_loader, old_test_loader, (mem_X, mem_y)


# ======================= è¯„ä¼°å·¥å…· =======================
@torch.no_grad()
def eval_old_only_before_train(model: OpenEmbed, test_old_loader: DataLoader, num_old: int, device: str):
    model.eval()
    tot, hit = 0, 0
    for x, y in test_old_loader:
        x = x.to(device)
        y = y.to(device)
        _, logits = model(x)
        pred = logits.argmax(1)
        hit += (pred == y).sum().item()
        tot += y.size(0)
    acc = hit / max(1, tot)
    return acc


@torch.no_grad()
def eval_teacher_upper_bound(cfg: Cfg):
    device = cfg.device
    num_old = len(cfg.known_classes)
    teacher = OpenEmbed(output=num_old).to(device)

    try:
        checkpoint = torch.load(cfg.teacher_model_path, map_location=device)

        # æ£€æŸ¥checkpointçš„ç»“æ„
        if 'model' in checkpoint:
            # å¦‚æœåŒ…å«'model'é”®ï¼Œæå–æ¨¡å‹æƒé‡
            teacher_sd = checkpoint['model']
        elif 'state_dict' in checkpoint:
            # å¦‚æœåŒ…å«'state_dict'é”®ï¼Œæå–æ¨¡å‹æƒé‡
            teacher_sd = checkpoint['state_dict']
        else:
            # å¦åˆ™å‡è®¾æ•´ä¸ªcheckpointå°±æ˜¯æ¨¡å‹æƒé‡
            teacher_sd = checkpoint

        # åŠ è½½æ¨¡å‹æƒé‡
        teacher.load_state_dict(teacher_sd)
        print(f"[Teacher] æˆåŠŸåŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼Œè¾“å‡ºç»´åº¦: {num_old}")

    except Exception as e:
        print(f"Error loading teacher model: {e}")
        print("Teacher evaluation skipped.")
        return

    # åŠ è½½æ—§ç±»æµ‹è¯•æ•°æ®
    X_old_test, y_old_test = load_all_data(cfg.data_root, cfg.known_classes)
    if len(X_old_test) == 0:
        print("No test data available for teacher evaluation")
        return

    test_loader = DataLoader(
        SignalDataset(X_old_test, y_old_test),
        batch_size=cfg.batch_size, shuffle=False
    )

    teacher.eval()
    tot, hit = 0, 0
    for x, y in test_loader:
        x = x.to(device)
        y = y.to(device)
        _, logits = teacher(x)
        pred = logits.argmax(1)
        hit += (pred == y).sum().item()
        tot += y.size(0)

    acc = hit / max(1, tot)
    print(f"[Teacher Upper Bound] æ—§ç±»åˆ«å‡†ç¡®ç‡: {acc:.4f} ({acc * 100:.2f}%)")


@torch.no_grad()
def evaluate_on_loaders(model: OpenEmbed, test_old_loader: DataLoader, new_test_loader: DataLoader, num_old: int,
                        device: str):
    model.eval()

    def _eval(loader):
        tot, hit = 0, 0
        for x, y in loader:
            x = x.to(device)
            y = y.to(device)
            _, logits = model(x)
            pred = logits.argmax(1)
            hit += (pred == y).sum().item()
            tot += y.size(0)
        return hit, tot

    old_hit, old_tot = _eval(test_old_loader)
    new_hit, new_tot = _eval(new_test_loader)

    acc_old = old_hit / max(1, old_tot)
    acc_new = new_hit / max(1, new_tot)
    acc_all = (old_hit + new_hit) / max(1, old_tot + new_tot)

    print(
        f"[Eval] æ—§ç±»åˆ«å‡†ç¡®ç‡: {acc_old * 100:.2f}% | æ–°ç±»åˆ«å‡†ç¡®ç‡: {acc_new * 100:.2f}% | æ€»ä½“å‡†ç¡®ç‡: {acc_all * 100:.2f}%")
    return {"old": acc_old, "new": acc_new, "overall": acc_all}


# ======================= A/GEM æ¢¯åº¦å®ç”¨ =======================
def flatten_grads(params):
    vec = []
    device = next((p.device for p in params if p is not None), torch.device("cpu"))
    for p in params:
        if p.grad is None:
            vec.append(torch.zeros_like(p).view(-1))
        else:
            vec.append(p.grad.view(-1))
    return torch.cat(vec) if len(vec) else torch.tensor([], device=device)


def load_to_grads(params, grad_vec):
    ptr = 0
    for p in params:
        n = p.numel()
        if p.grad is None:
            p.grad = torch.zeros_like(p.data)
        p.grad.view(-1).copy_(grad_vec[ptr:ptr + n])
        ptr += n


@contextmanager
def stable_layers(model):
    # å†»ç»“ BN running stats + å…³é—­ Dropoutï¼Œä»…åœ¨ä¸¤æ¬¡ backward æœŸé—´
    bns, dos = [], []
    for m in model.modules():
        if isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d)):
            bns.append((m, m.training))
            m.eval()
        if isinstance(m, (nn.Dropout, nn.Dropout2d, nn.Dropout3d)):
            dos.append((m, m.training))
            m.eval()
    try:
        yield
    finally:
        for m, was_train in bns + dos:
            m.train(was_train)


# ======================= GEM æŠ•å½±ï¼ˆK=1 é—­å¼ï¼›K>1 QP/PGDï¼‰ =======================
def gem_project(g_new: torch.Tensor, mem_grads: torch.Tensor, margin: float = 0.0) -> torch.Tensor:
    """
    g_new: [P]
    mem_grads: [K, P]  (Kä¸ªçº¦æŸå‘é‡)
    return: g_proj: [P]
    """
    if mem_grads.numel() == 0:
        return g_new

    # K=1ï¼šé—­å¼è§£
    if mem_grads.dim() == 1 or mem_grads.shape[0] == 1:
        m = mem_grads.view(-1)
        den = torch.dot(m, m)
        if den.item() == 0:
            return g_new
        viol = torch.dot(g_new, m) - margin
        if viol.item() < 0:
            return g_new - (viol / den) * m
        return g_new

    # K>1ï¼šç®€åŒ–ç‰ˆæœ¬ï¼Œé¿å… quadprog ä¾èµ–
    # ä½¿ç”¨ PGD on dual
    M = mem_grads  # [K,P]
    P = M @ M.t()  # [K,K]
    q = -(M @ g_new)  # [K]

    # ä¼°è®¡ Lipschitz å¸¸æ•° L = ||P||_2
    def power_iter(A, iters=20):
        v = torch.randn(A.shape[1], device=A.device)
        v = v / (v.norm() + 1e-12)
        for _ in range(iters):
            v = A @ (A.t() @ v)
            v = v / (v.norm() + 1e-12)
        Av = A @ v
        return (Av.norm() / (v.norm() + 1e-12)).item()

    L = max(1.0, power_iter(P))
    step = 1.0 / L
    v = torch.clamp_min(torch.zeros(P.shape[0], device=g_new.device), margin)

    for _ in range(100):
        grad_v = P @ v + q  # âˆ‡
        v = v - step * grad_v
        v = torch.clamp_min(v, margin)

    x = M.t() @ v + g_new
    return x


# ======================= è®­ç»ƒï¼ˆGEMï¼‰ =======================
def train_gem(cfg: Cfg):
    os.makedirs(cfg.out_dir, exist_ok=True)
    device = cfg.device
    num_old = len(cfg.known_classes)
    num_new = len(cfg.new_class_labels)
    total_classes = num_old + num_new

    print(f"[Training] æ€»ç±»åˆ«æ•°: {total_classes} (æ—§: {num_old}, æ–°: {num_new})")

    new_loader, new_test_loader, test_old_loader, (mem_X, mem_y) = build_loaders_and_memory(cfg)

    # åˆå§‹åŒ–å¯è§†åŒ–å·¥å…·
    visualizer = Visualization(cfg)

    # å­¦ç”Ÿæ¨¡å‹ï¼šåŠ è½½æ•™å¸ˆéª¨å¹² + æ‰©å±•åˆ†ç±»å¤´
    model = OpenEmbed(output=total_classes).to(device)

    # åŠ è½½é¢„è®­ç»ƒæƒé‡
    try:
        checkpoint = torch.load(cfg.teacher_model_path, map_location=device)

        # æ£€æŸ¥checkpointçš„ç»“æ„å¹¶æå–æ¨¡å‹æƒé‡
        if 'model' in checkpoint:
            teacher_sd = checkpoint['model']
            print("[Init] ä»checkpointä¸­æå–'model'æƒé‡")
        elif 'state_dict' in checkpoint:
            teacher_sd = checkpoint['state_dict']
            print("[Init] ä»checkpointä¸­æå–'state_dict'æƒé‡")
        else:
            teacher_sd = checkpoint
            print("[Init] ä½¿ç”¨æ•´ä¸ªcheckpointä½œä¸ºæ¨¡å‹æƒé‡")

        # å¤åˆ¶é™¤äº†æœ€åä¸€å±‚å¤–çš„æ‰€æœ‰æƒé‡
        model_state = model.state_dict()
        loaded_keys = []

        for key in teacher_sd:
            if key in model_state:
                if model_state[key].shape == teacher_sd[key].shape:
                    model_state[key] = teacher_sd[key]
                    loaded_keys.append(key)
                else:
                    print(f"[Init] è·³è¿‡æƒé‡ {key}: å½¢çŠ¶ä¸åŒ¹é… {teacher_sd[key].shape} vs {model_state[key].shape}")
            else:
                print(f"[Init] è·³è¿‡æœªçŸ¥é”®: {key}")

        model.load_state_dict(model_state)
        print(f"[Init] æˆåŠŸåŠ è½½ {len(loaded_keys)} ä¸ªæƒé‡å‚æ•°")

    except Exception as e:
        print(f"Error loading teacher model: {e}")
        print("Training from scratch...")

    # åªè®­ç»ƒæœ€åä¸€å±‚ï¼ˆåˆ†ç±»å¤´ï¼‰
    for n, p in model.named_parameters():
        p.requires_grad = n.startswith("fc")  # æ ¹æ®æ‚¨çš„æ¨¡å‹ç»“æ„è°ƒæ•´

    # ä¼˜åŒ–å™¨ + è°ƒåº¦
    if cfg.optimizer_type.lower() == "sgd":
        optimizer = SGD((p for p in model.parameters() if p.requires_grad),
                        lr=cfg.lr, momentum=cfg.momentum, nesterov=cfg.nesterov,
                        weight_decay=cfg.weight_decay)
    else:
        optimizer = AdamW((p for p in model.parameters() if p.requires_grad),
                          lr=cfg.lr, weight_decay=cfg.weight_decay)
    scheduler = CosineAnnealingLR(optimizer, T_max=cfg.epochs)
    ce = nn.CrossEntropyLoss(label_smoothing=cfg.label_smoothing)

    # è®­ç»ƒå‰è¯„ä¼°
    if cfg.show_teacher_upper_bound:
        eval_teacher_upper_bound(cfg)
    _ = eval_old_only_before_train(model, test_old_loader, num_old, device)

    trainable_params = [p for p in model.parameters() if p.requires_grad]

    print("\n===== å¼€å§‹ GEM è®­ç»ƒ =====")
    for epoch in range(cfg.epochs):
        model.train()
        running_loss = 0.0
        steps = len(new_loader)

        for step, (x_new, y_new) in enumerate(new_loader, start=1):
            x_new, y_new = x_new.to(device), y_new.to(device)
            K = max(1, int(cfg.gem_num_ref_batches))
            ref_grads = []

            with stable_layers(model):
                # --- æ–°æ¢¯åº¦ ---
                optimizer.zero_grad(set_to_none=True)
                _, logits_new = model(x_new)
                loss_new = ce(logits_new, y_new)
                loss_new.backward()
                g_new = flatten_grads(trainable_params)

                # --- å‚è€ƒæ¢¯åº¦ï¼ˆæ—§æ ·æœ¬ï¼‰---
                if mem_X.numel() > 0:
                    for _ in range(K):
                        bs_ref = min(x_new.shape[0], mem_X.shape[0])
                        # å…³é”®ä¿®å¤ï¼šç´¢å¼•å¿…é¡»åœ¨CPUä¸Šï¼Œå› ä¸ºmem_Xåœ¨CPUä¸Š
                        idx = torch.randint(0, mem_X.shape[0], (bs_ref,), device="cpu")
                        x_old = mem_X[idx].to(device, non_blocking=True)
                        y_old = mem_y[idx].to(device, non_blocking=True)

                        optimizer.zero_grad(set_to_none=True)
                        _, logits_old = model(x_old)
                        loss_old = ce(logits_old, y_old)
                        loss_old.backward()
                        ref_grads.append(flatten_grads(trainable_params))

            # --- GEM æŠ•å½±å¹¶æ›´æ–° ---
            M = torch.stack(ref_grads, dim=0) if len(ref_grads) else torch.empty(0, device=device)
            g_proj = gem_project(g_new, M, margin=cfg.gem_margin) if M.numel() else g_new
            optimizer.zero_grad(set_to_none=True)
            load_to_grads(trainable_params, g_proj)
            optimizer.step()
            running_loss += float(loss_new.item())

            if step % cfg.print_every == 0:
                print(f'Epoch [{epoch + 1}/{cfg.epochs}], Step [{step}/{steps}], Loss: {loss_new.item():.4f}')

        avg_loss_epoch = running_loss / max(1, steps)
        current_lr = scheduler.get_last_lr()[0]

        # è®°å½•è®­ç»ƒæŸå¤±
        visualizer.add_training_loss(epoch + 1, avg_loss_epoch)

        print(f"==> Epoch {epoch + 1}/{cfg.epochs} | Mean New Loss: {avg_loss_epoch:.4f} | LR: {current_lr:.6f}")
        scheduler.step()

        if cfg.do_eval_during_train and ((epoch + 1) % cfg.eval_every == 0):
            metrics = evaluate_on_loaders(model, test_old_loader, new_test_loader, num_old, device)
            visualizer.add_eval_metrics(epoch + 1, metrics)

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    torch.save(model.state_dict(), cfg.final_model)
    print(f"[Save] æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {cfg.final_model}")

    # æœ€ç»ˆè¯„ä¼°
    final_metrics = evaluate_on_loaders(model, test_old_loader, new_test_loader, num_old, device)

    # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
    print("\n" + "=" * 60)
    print("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    print("=" * 60)

    # 1. ç»˜åˆ¶è®­ç»ƒæŸå¤±æ›²çº¿
    visualizer.plot_training_loss()

    # 2. ç»˜åˆ¶å‡†ç¡®ç‡æ¼”åŒ–æ›²çº¿
    visualizer.plot_accuracy_evolution()

    # 3. ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    confusion_mat = visualizer.plot_confusion_matrix(model, test_old_loader, new_test_loader, device)

    # 4. æ‰“å°æœ€ç»ˆæŠ¥å‘Š
    visualizer.print_final_report(final_metrics, confusion_mat)

    return final_metrics


# ======================= ä¸»å…¥å£ =======================
if __name__ == "__main__":
    cfg = Cfg()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(cfg.out_dir, exist_ok=True)
    os.makedirs(cfg.vis_dir, exist_ok=True)

    print("=" * 80)
    print("USTC-TFC2016 å¢é‡å­¦ä¹ å®éªŒ")
    print(f"å·²çŸ¥ç±»åˆ«: {cfg.known_classes}")
    print(f"æ–°å¢ç±»åˆ«: {cfg.new_class_labels}")
    print(f"è®°å¿†æ ·æœ¬æ•°/ç±»: {cfg.memory_per_class}")
    print(f"è®¾å¤‡: {cfg.device}")
    print("=" * 80)

    try:
        metrics = train_gem(cfg)
        print(f"\nå¢åŠ {cfg.new_class_labels}ç§æœªçŸ¥æ¶æ„æµé‡çš„å‡†ç¡®ç‡: {metrics['new']}")
    except Exception as e:
        print(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        import traceback

        traceback.print_exc()